{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 01 -- Tokenize word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing : work tokenizer & Sentence tokenizer\n",
    "\n",
    "# lexicon & corporas\n",
    "# corpora = Body of text ie, medical journal , presidential speeches , English Language\n",
    "# lexicon = word and their meaning \n",
    "\n",
    "# Investor Speak & Regular English Speak\n",
    "# Investor Speak \"bull\" - someone who positive about the market\n",
    "# English Speak \"bull\" - scary animal you don't want running @ you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello Mr. Khushal Bisani, What are you doing? It's time to go for breakfast. Are you ready?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Khushal Bisani, What are you doing?',\n",
       " \"It's time to go for breakfast.\",\n",
       " 'Are you ready?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tok = sent_tokenize(example)\n",
    "sent_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Khushal',\n",
       " 'Bisani',\n",
       " ',',\n",
       " 'What',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'time',\n",
       " 'to',\n",
       " 'go',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " '.',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'ready',\n",
       " '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tok = word_tokenize(example)\n",
    "word_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 02 -- Stopwords - like, is, are, am, he, she, that, why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Mr. Khushal Bisani, What are you doing? It's time to go for breakfast. Are you ready?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) #german,french & portugese\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Khushal',\n",
       " 'Bisani',\n",
       " ',',\n",
       " 'What',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'time',\n",
       " 'to',\n",
       " 'go',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " '.',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'ready',\n",
       " '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(example)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Khushal',\n",
       " 'Bisani',\n",
       " ',',\n",
       " 'What',\n",
       " '?',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'time',\n",
       " 'go',\n",
       " 'breakfast',\n",
       " '.',\n",
       " 'Are',\n",
       " 'ready',\n",
       " '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence=[]\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_sentence.append(word)\n",
    "        \n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 3 -- Stemming - [\"ask\",\"asking\",\"asked\"] -> [\"ask\",\"ask\",\"ask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask\n",
      "ask\n",
      "ask\n",
      "askdf\n",
      "\n",
      "hello\n",
      "man\n",
      ",\n",
      "you\n",
      "ask\n",
      "me\n",
      "to\n",
      "use\n",
      "python\n",
      "with\n",
      "keen\n",
      "pythonli\n",
      ",\n",
      "now\n",
      "i\n",
      "am\n",
      "ask\n",
      "to\n",
      "you\n",
      "who\n",
      "are\n",
      "you\n",
      "to\n",
      "ask\n",
      "me\n",
      "that\n",
      "how\n",
      "am\n",
      "i\n",
      "use\n",
      "python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_word = [\"ask\",\"asked\",\"asking\",\"askdf\"]\n",
    "\n",
    "for i in example_word:\n",
    "    print(ps.stem(i))\n",
    "\n",
    "print()\n",
    "sent = \"hello man, you asked me to use python with keen pythonly,\\\n",
    "now i am asking to you who are you to ask me that how am i use pythoned.\"\n",
    "\n",
    "words = word_tokenize(sent)\n",
    "\n",
    "for j in words:\n",
    "    print(ps.stem(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 4 -- speech tagging"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"C:\\\\Users\\\\khush\\\\Documents\\\\abc.txt\")\n",
    "sample_text = state_union.raw(\"C:\\\\Users\\\\khush\\\\Documents\\\\xyz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_set_tokenizer = PunktSentenceTokenizer(train_text) #train\n",
    "tokenizer = custom_set_tokenizer.tokenize(sample_text) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenizer[:5]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('This', 'DT'), ('is', 'VBZ'), ('president', 'NN'), ('of', 'IN'), ('India', 'NNP'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Narendra', 'NNP'), ('Modi', 'NNP'), ('.', '.')]\n",
      "[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('discuss', 'VB'), ('on', 'IN'), ('the', 'DT'), ('economy', 'NN'), ('of', 'IN'), ('india', 'NN'), ('.', '.')]\n",
      "[('India', 'NNP'), ('already', 'RB'), ('in', 'IN'), ('depth', 'NN'), ('.', '.')]\n",
      "[('So', 'RB'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('that', 'WDT'), ('you', 'PRP'), ('as', 'IN'), ('acitizen', 'NNS'), ('of', 'IN'), ('india', 'NNS'), ('take', 'VBP'), ('a', 'DT'), ('sharp', 'JJ'), ('look', 'NN'), ('towards', 'IN'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('state', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('getting', 'VBG'), ('worse', 'JJR'), ('day', 'NN'), ('by', 'IN'), ('day', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 5 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Hello/NNP)\n",
      "  ,/,\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  president/NN\n",
      "  of/IN\n",
      "  (Chunk India/NNP)\n",
      "  ./.)\n",
      "(S My/PRP$ name/NN (Chunk is/VBZ Narendra/NNP Modi/NNP) ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  discuss/VB\n",
      "  on/IN\n",
      "  the/DT\n",
      "  economy/NN\n",
      "  of/IN\n",
      "  india/NN\n",
      "  ./.)\n",
      "(S (Chunk India/NNP) already/RB in/IN depth/NN ./.)\n",
      "(S\n",
      "  So/RB\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  that/WDT\n",
      "  you/PRP\n",
      "  as/IN\n",
      "  acitizen/NNS\n",
      "  of/IN\n",
      "  india/NNS\n",
      "  take/VBP\n",
      "  a/DT\n",
      "  sharp/JJ\n",
      "  look/NN\n",
      "  towards/IN\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  state/NN\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  getting/VBG\n",
      "  worse/JJR\n",
      "  day/NN\n",
      "  by/IN\n",
      "  day/NN\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  follow/VB\n",
      "  few/JJ\n",
      "  rules/NNS\n",
      "  for/IN\n",
      "  young/JJ\n",
      "  (Chunk India/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  These/DT\n",
      "  rules/NNS\n",
      "  are/VBP\n",
      "  plant/NN\n",
      "  plants/NNS\n",
      "  in/IN\n",
      "  their/PRP$\n",
      "  10/CD\n",
      "  percent/JJ\n",
      "  area/NN\n",
      "  of/IN\n",
      "  home/NN\n",
      "  ,/,\n",
      "  birth/JJ\n",
      "  rate/NN\n",
      "  control/NN\n",
      "  to/TO\n",
      "  2/CD\n",
      "  per/IN\n",
      "  woman/NN\n",
      "  ,/,\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  waste/VB\n",
      "  excess/JJ\n",
      "  water/NN\n",
      "  ,/,\n",
      "  try/NN\n",
      "  to/TO\n",
      "  use/VB\n",
      "  public/JJ\n",
      "  transport/NN\n",
      "  facility/NN\n",
      "  instead/RB\n",
      "  of/IN\n",
      "  private/JJ\n",
      "  in/IN\n",
      "  other/JJ\n",
      "  words/NNS\n",
      "  focus/VBN\n",
      "  on/IN\n",
      "  less/JJR\n",
      "  polution/NN\n",
      "  and/CC\n",
      "  low/JJ\n",
      "  usage/NN\n",
      "  of/IN\n",
      "  fuel/NN\n",
      "  ./.)\n",
      "(S (Chunk Thankyou/NNP India/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "def process_content1():\n",
    "    try:\n",
    "        for i in tokenizer:\n",
    "            word = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(word)\n",
    "            chunkgram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkgram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "# + = match 1 or more\n",
    "# ? = match 0 or 1 repetitions.\n",
    "# * = match 0 or MORE repetitions \n",
    "# . = Any character except a new line\n",
    "\n",
    "process_content1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 6 Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Hello/NNP ,/,)\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  (Chunk president/NN)\n",
      "  of/IN\n",
      "  (Chunk India/NNP ./.))\n",
      "(S (Chunk My/PRP$ name/NN) is/VBZ (Chunk Narendra/NNP Modi/NNP ./.))\n",
      "(S\n",
      "  (Chunk I/PRP)\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  discuss/VB\n",
      "  on/IN\n",
      "  the/DT\n",
      "  (Chunk economy/NN)\n",
      "  of/IN\n",
      "  (Chunk india/NN ./.))\n",
      "(S (Chunk India/NNP already/RB) in/IN (Chunk depth/NN ./.))\n",
      "(S\n",
      "  (Chunk So/RB ,/, I/PRP)\n",
      "  want/VBP\n",
      "  (Chunk that/WDT you/PRP)\n",
      "  as/IN\n",
      "  (Chunk acitizen/NNS)\n",
      "  of/IN\n",
      "  (Chunk india/NNS)\n",
      "  take/VBP\n",
      "  a/DT\n",
      "  (Chunk sharp/JJ look/NN)\n",
      "  towards/IN\n",
      "  (Chunk it/PRP)\n",
      "  's/VBZ\n",
      "  (Chunk state/NN which/WDT)\n",
      "  is/VBZ\n",
      "  getting/VBG\n",
      "  (Chunk worse/JJR day/NN)\n",
      "  by/IN\n",
      "  (Chunk day/NN ./.))\n",
      "(S\n",
      "  (Chunk I/PRP)\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  follow/VB\n",
      "  (Chunk few/JJ rules/NNS)\n",
      "  for/IN\n",
      "  (Chunk young/JJ India/NNP ./.))\n",
      "(S\n",
      "  These/DT\n",
      "  (Chunk rules/NNS)\n",
      "  are/VBP\n",
      "  (Chunk plant/NN plants/NNS)\n",
      "  in/IN\n",
      "  (Chunk their/PRP$ 10/CD percent/JJ area/NN)\n",
      "  of/IN\n",
      "  (Chunk home/NN ,/, birth/JJ rate/NN control/NN)\n",
      "  to/TO\n",
      "  (Chunk 2/CD)\n",
      "  per/IN\n",
      "  (Chunk woman/NN ,/,)\n",
      "  do/VBP\n",
      "  (Chunk n't/RB)\n",
      "  waste/VB\n",
      "  (Chunk excess/JJ water/NN ,/, try/NN)\n",
      "  to/TO\n",
      "  use/VB\n",
      "  (Chunk public/JJ transport/NN facility/NN instead/RB)\n",
      "  of/IN\n",
      "  (Chunk private/JJ)\n",
      "  in/IN\n",
      "  (Chunk other/JJ words/NNS)\n",
      "  focus/VBN\n",
      "  on/IN\n",
      "  (Chunk less/JJR polution/NN and/CC low/JJ usage/NN)\n",
      "  of/IN\n",
      "  (Chunk fuel/NN ./.))\n",
      "(S (Chunk Thankyou/NNP India/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "def process_content2():\n",
    "    try:\n",
    "        for i in tokenizer:\n",
    "            word = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(word)\n",
    "            chunkgram = r\"\"\"Chunk: {<.*>+}\n",
    "                                         }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkgram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "# + = match 1 or more\n",
    "# ? = match 0 or 1 repetitions.\n",
    "# * = match 0 or MORE repetitions \n",
    "# . = Any character except a new line\n",
    "\n",
    "process_content2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 7 - Named Entity REcognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hello/NNP)\n",
      "  ,/,\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  president/NN\n",
      "  of/IN\n",
      "  (GPE India/NNP)\n",
      "  ./.)\n",
      "(S My/PRP$ name/NN is/VBZ (PERSON Narendra/NNP Modi/NNP) ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  discuss/VB\n",
      "  on/IN\n",
      "  the/DT\n",
      "  economy/NN\n",
      "  of/IN\n",
      "  india/NN\n",
      "  ./.)\n",
      "(S (GPE India/NNP) already/RB in/IN depth/NN ./.)\n",
      "(S\n",
      "  So/RB\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  that/WDT\n",
      "  you/PRP\n",
      "  as/IN\n",
      "  acitizen/NNS\n",
      "  of/IN\n",
      "  india/NNS\n",
      "  take/VBP\n",
      "  a/DT\n",
      "  sharp/JJ\n",
      "  look/NN\n",
      "  towards/IN\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  state/NN\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  getting/VBG\n",
      "  worse/JJR\n",
      "  day/NN\n",
      "  by/IN\n",
      "  day/NN\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  follow/VB\n",
      "  few/JJ\n",
      "  rules/NNS\n",
      "  for/IN\n",
      "  young/JJ\n",
      "  (GPE India/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  These/DT\n",
      "  rules/NNS\n",
      "  are/VBP\n",
      "  plant/NN\n",
      "  plants/NNS\n",
      "  in/IN\n",
      "  their/PRP$\n",
      "  10/CD\n",
      "  percent/JJ\n",
      "  area/NN\n",
      "  of/IN\n",
      "  home/NN\n",
      "  ,/,\n",
      "  birth/JJ\n",
      "  rate/NN\n",
      "  control/NN\n",
      "  to/TO\n",
      "  2/CD\n",
      "  per/IN\n",
      "  woman/NN\n",
      "  ,/,\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  waste/VB\n",
      "  excess/JJ\n",
      "  water/NN\n",
      "  ,/,\n",
      "  try/NN\n",
      "  to/TO\n",
      "  use/VB\n",
      "  public/JJ\n",
      "  transport/NN\n",
      "  facility/NN\n",
      "  instead/RB\n",
      "  of/IN\n",
      "  private/JJ\n",
      "  in/IN\n",
      "  other/JJ\n",
      "  words/NNS\n",
      "  focus/VBN\n",
      "  on/IN\n",
      "  less/JJR\n",
      "  polution/NN\n",
      "  and/CC\n",
      "  low/JJ\n",
      "  usage/NN\n",
      "  of/IN\n",
      "  fuel/NN\n",
      "  ./.)\n",
      "(S (GPE Thankyou/NNP) (GPE India/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_file = state_union.raw(\"C:\\\\Users\\\\khush\\\\Documents\\\\abc.txt\")\n",
    "sample_file = state_union.raw(\"C:\\\\Users\\\\khush\\\\Documents\\\\xyz.txt\")\n",
    "\n",
    "custom_set_tokenizer = PunktSentenceTokenizer(train_file)\n",
    "sample_set_tokenizer = custom_set_tokenizer.tokenize(sample_file)\n",
    "\n",
    "def process3():\n",
    "    try:\n",
    "        for i in sample_set_tokenizer:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged , binary = False)\n",
    "            print(namedEnt)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    " \n",
    "\n",
    "process3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 8 - Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "good\n",
      "asked\n",
      "ask\n",
      "mouse\n",
      "runner\n",
      "cloudy\n",
      "hangup\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"better\" , pos = \"a\"))\n",
    "print(lemmatizer.lemmatize(\"asked\"))\n",
    "print(lemmatizer.lemmatize(\"asked\",pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"mice\"))\n",
    "print(lemmatizer.lemmatize(\"runner\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"cloudy\"))\n",
    "print(lemmatizer.lemmatize(\"hangup\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 9 -Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the morning were the first day.\n",
      "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
      "and let it divide the waters from the waters.\n",
      "1:7 And God made the firmament, and divided the waters which were\n",
      "under the firmament from the waters which were above the firmament:\n",
      "and it was so.\n",
      "1:8 And God called the firmament Heaven.\n"
     ]
    }
   ],
   "source": [
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for i in tok[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - 10 - Wordnet nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n",
      "program\n",
      "programme\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())\n",
    "print(syns[0].lemmas()[1].name())\n",
    "print(syns[0].lemmas()[2].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plan.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('plan.n.01.plan')\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a radio or television show\n"
     ]
    }
   ],
   "source": [
    "print(syns[2].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did you see his program last night?']\n"
     ]
    }
   ],
   "source": [
    "print(syns[2].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "print(synonyms)\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('program.n.02')\n",
    "w2 = wordnet.synset('broadcast.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('plan.n.02')\n",
    "w2 = wordnet.synset('program.n.02')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
